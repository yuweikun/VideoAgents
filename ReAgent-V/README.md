# ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding

<p align="center">
  <a href="https://github.com/YiyangZhou">Yiyang Zhou*</a>, 
  <a href="https://github.com/codepassionor">Yangfan He*</a>, 
  <a href="https://github.com/InfiniteLoopCoder">Yaofeng Su</a>, 
  <a href="https://github.com/Lillianwei-h">Siwei Han</a>, 
  <a href="https://github.com/joeljang">Joel Jang</a>, 
  <a href="https://www.gedasbertasius.com/">Gedas Bertasius</a>, 
  <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>, 
  <a href="https://www.huaxiuyao.io/">Huaxiu Yao</a>
</p>


<div align="center">
*Equal Contribution
</div>

<div align="center">
    <a href="https://arxiv.org/abs/2506.01300"><img src="assets/Paper-Arxiv-orange.svg" ></a>
</div>


**ReAgent-V** is a **modular**, **extensible**, and **reward-aware** video reasoning framework designed to elevate video question answering and reasoning through:

- ğŸ”§ **Flexible Tool Integration** â€” Plug-and-play support for OCR, ASR, object detection, scene graph generation, captioning, and more  
- ğŸ§  **Reward-Guided Inference** â€” Enables real-time self-correction via structured reward signals  
- ğŸ¯ **Adaptive Model Alignment** â€” Aligns models dynamically based on inference-time feedback  
- ğŸ—‚ï¸ **High-Quality Data Selection** â€” Facilitates sample-efficient learning using reflective evaluation  
- ğŸ“Š **Entropy-Calibrated Frame Selection** â€” Prioritizes key frames for focused reasoning  
- ğŸ” **Multi-Perspective Reflection** â€” Refines answers through debate among conservative, neutral, and aggressive viewpoints

  
## News
ğŸš€ **Update [Sept 19, 2025]:** Our paper (arXiv:2506.01300) has been accepted to **NeurIPS 2025**! 

ğŸ”¥ **\[June 2, 2025]** Our latest paper is now live on arXiv: [arXiv:2506.01300](https://arxiv.org/abs/2506.01300)!


---

# ğŸ“Œ Overview
![Framework Overview](assets/framework.png)  


## ğŸš€ Applications

ReAgent-V supports a range of real-world tasks via dedicated application modules:

### ğŸ§­ VLA Alignment  
Aligns **Vision-Language-Action (VLA)** models using **Trajectory-wise Preference Optimization (TPO)** guided by ReAgent-Vâ€™s reward feedback. Specifically, ReAgent-V evaluates each trajectory across multiple axesâ€”such as task success, temporal stability, visual grounding, and semantic precisionâ€”and performs multi-agent reflection to produce refined, high-fidelity reward scores for alignment.

- ğŸ“ Module: `Application/VLA-Alignment`  
- ğŸ“˜ Instructions: [VLA Alignment README](https://github.com/aiming-lab/ReAgent-V/blob/main/Application/VLA-Alignment/README.md)

### ğŸ¥ Video Understanding 

- **Entropy-Calibrated Frame Selection**  
  Efficiently selects the most informative frames for video reasoning.

- **Tool-Augmented Inference**  
  Dynamically integrates multimodal tools including OCR, ASR, object detection, scene graph generation, and captioning, etc.

- **Multi-Agent Reflection**  
  Iteratively refines outputs by encouraging disagreement and consensus among diverse agent personas (conservative / neutral / aggressive).

- ğŸ“ Module: `ReAgent-V`  
- ğŸ“˜ Instructions: [Video Understanding README](https://github.com/aiming-lab/ReAgent-V/blob/main/ReAgent-V/README.md)

### ğŸ“ˆ Reward-Aware Data Curation and Collection for SFT, DPO, GRPO, and Beyond

ReAgent-V enables **inference-time data curation** by leveraging real-time rewards and reflection-based diagnostics depending on the optimization paradigm, the extraction strategy varies:


#### ğŸ§ª For **SFT (Supervised Fine-Tuning)**

ReAgent-V can directly **collect samples with high reward scores** (from the evaluation report) without requiring additional reflection.

* âœ… These samples indicate that the modelâ€™s initial reasoning is reliable.
* ğŸ“¥ Stored as supervised training pairs with accompanying scalar reward labels from the critic agent.

> Simple, scalable, and label-efficient: reward scores enable dynamic filtering without manual annotation.


#### ğŸ”„ For **GRPO (Group Relative Policy Optimization)**

To curate high-value training data for GRPO, ReAgent-V employs a **reflection-triggering mechanism** grounded in **importance scoring**, effectively identifying **challenging yet informative** video-text samples during the *video understanding phase*.

* ğŸ¥ Each input is a **(video, text) pair**, typically comprising a video and its initial response.
* ğŸ“Š During inference, ReAgent-V computes an **importance score** (denoted as `E.importance_score`) based on the critic agentâ€™s **overall assessment of reasoning sufficiency**.
* â— If this importance score falls **below a threshold** (e.g., `< 5 out of 10`), the sample is considered **difficult**, meaning the model struggled with initial reasoning and likely required further refinement.
* ğŸ“¥ The resulting **(video, text)** samples are labeled as **reflection-worthy** and collected as **valuable candidates** for GRPO training.


#### âš–ï¸ For **DPO (Direct Preference Optimization)**

ReAgent-V supports **Direct Preference Optimization (DPO)** by reframing itself from a video reasoning agent into a **reward-generating agent**. This is achieved through a **task template modification** that emphasizes **evaluating answer quality** rather than producing a single correct answer.

* ğŸ§  Transforms the task from â€œanswer the video questionâ€ into **â€œscore the video from different perspectivesâ€** based on visual evidence.

* â™»ï¸ Uses multi-perspective reflection outputs (e.g., **conservative**, **neutral**, **aggressive**) to generate **candidate rewards**.

* ğŸ“Š Each candidate reward is generated along **customizable reward dimensions**, such as:

  * ğŸ¯ Visual alignment
  * â±ï¸ Temporal accuracy
  * ğŸ’¬ Linguistic precision
  * ğŸ§  Reasoning specificity
  * ğŸ” Option disambiguation

* âœ… The system identifies the answer with the **higher reflection reward** as the preferred choice.

* ğŸ”— Constructs (preferred, rejected) pairs from these outputs to serve as **DPO training data**.

> Unlike static or hand-crafted rewards, ReAgent-Vâ€™s feedback is **context-aware, multi-dimensional, and fully dynamic**, adapting to each video-question instance.


---

> ğŸŒ€ **Unified Insight**:
> ReAgent-V closes the data curation loop. Its multi-agent reward pipeline not only improves current inference but **continuously supplies high-fidelity data** for future optimizationâ€”making learning systems **self-refining** in the wild.

- ğŸ“ Module: `ReAgent-V`  
- ğŸ“˜ Instructions: [Video Understanding README](https://github.com/aiming-lab/ReAgent-V/blob/main/ReAgent-V/README.md)

  
## ğŸ§‘â€ğŸ’» Getting Started

Each subfolder contains its own `README.md` with detailed installation, setup, and training instructions. To get started:

1. Clone the repository  
2. Follow the environment setup and requirements in each module  
3. Explore the demo scripts and customize as needed

ğŸ’¬ If you have questions or encounter any issues, feel free to open an [issue](https://github.com/aiming-lab/ReAgent-V/issues) or contact the maintainers.

---

## ğŸ“š Citation

If you find ReAgent-V helpful in your research or projects, please consider citing:

```bibtex
@article{zhou2025reagent,
  title={ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding},
  author={Zhou, Yiyang and He, Yangfan and Su, Yaofeng and Han, Siwei and Jang, Joel and Bertasius, Gedas and Bansal, Mohit and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2506.01300},
  year={2025}
}
