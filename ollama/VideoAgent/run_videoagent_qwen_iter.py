"""VideoAgent-style iterative refinement using Qwen3-VL-Flash (same model/key as ReAgent).

Inputs:
- sample_data videos (mp4) + vehicle_dynamics_data/*.csv + vehicle_gaze_data/*.csv + labels.json
- template.py (one_shot + INSTRUCTION) for structured 5-section driver report.

Loop:
0) Draft report from video + signals.
1) Critic scores (eval_reward_prompt_template) -> scalar_reward.
2) If reward < threshold, rewrite using critic feedback; repeat up to max_iter.
3) Final format enforcement: five headings, top-3 actions from provided action set.
"""

from __future__ import annotations

import argparse
import csv
import json
import os
import re
import tempfile
import uuid
from pathlib import Path
from typing import Any, Dict, List

import imageio.v3 as iio
import requests
from openai import OpenAI

# Ensure imports work when run from repo root
ROOT = Path(__file__).resolve().parent.parent
REPO_ROOT = ROOT.parent
PKG_ROOT = Path(__file__).resolve().parent
for p in (ROOT, REPO_ROOT, PKG_ROOT):
    if str(p) not in os.sys.path:
        os.sys.path.append(str(p))

from template import INSTRUCTION, one_shot  # type: ignore

# Local copy of critic prompt（避免依赖 ReAgent-V）
eval_reward_prompt_template = """
[Task]  
You are a critic agent tasked with evaluating the quality of the initial answer A_0 generated by the target agent, using the given question and contextual information. Your goal is to provide structured diagnostic feedback by scoring the answer across multiple dimensions and computing a final scalar reward (0.0~10.0) based on the total score.

[Input Data]  
- Question: {question}  
- Context: {context}  
- Initial Answer: {initial_answer}

[Evaluation Criteria]  
Rate the answer on the following five dimensions (0.0~10.0 scale for each):

1. Visual Alignment: Is the answer aligned with visible video evidence?  
2. Temporal Accuracy: Is the answer consistent with the timeline or timestamps?  
3. Option Disambiguation: If multiple options are similar, does the answer clearly justify the selected one?  
4. Reasoning Specificity: Is the reasoning clear, focused, and appropriately detailed?  
5. Linguistic Precision: Is the answer grammatically correct and semantically accurate?

[Output Format]  
Return a JSON object with the following fields:  
{{  
  "structured_feedback": "...",  
  "scores": {{  
    "visual_alignment": {{ "value": float, "reason": "..." }},  
    "temporal_accuracy": {{ "value": float, "reason": "..." }},  
    "option_disambiguation": {{ "value": float, "reason": "..." }},  
    "reasoning_specificity": {{ "value": float, "reason": "..." }},  
    "linguistic_precision": {{ "value": float, "reason": "..." }}  
  }},  
  "total_score": float,  
  "scalar_reward": float  
}}  
"""

SAMPLE_ROOT = REPO_ROOT / "sample_data"

DEFAULT_MODEL = "qwen3-vl:2b"
DEFAULT_BASE_URL = "http://localhost:11434/v1"
DEFAULT_NUM_FRAMES = 4
DEFAULT_REWARD_THRESHOLD = 7.0
DEFAULT_MAX_ITER = 2

DEFAULT_ACTION_POOL = [
    "Focus on traffic",
    "Continue",
    "Prepare takeover",
    "Check mirrors",
    "Decelerate",
    "Engage manual",
    "Observe pedestrians",
    "Reduce speed",
]


def load_labels() -> List[Dict[str, Any]]:
    with open(SAMPLE_ROOT / "labels.json", "r", encoding="utf-8") as f:
        return json.load(f)


def find_label(folder: str, file_stem: str) -> Dict[str, Any]:
    target = f"{file_stem}.txt"
    for item in load_labels():
        if item.get("folder_name") == folder and item.get("file_name") == target:
            return item
    raise FileNotFoundError(f"label not found for {folder}/{target}")


def load_csv_rows(path: Path) -> List[Dict[str, str]]:
    with open(path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        return list(reader)


def sample_series(series: List[float], target_len: int = 30) -> List[float]:
    if not series:
        return []
    if len(series) <= target_len:
        return [round(x, 3) for x in series]
    step = max(1, len(series) // target_len)
    return [round(series[i], 3) for i in range(0, len(series), step)][:target_len]


def build_payload(folder: str, file_stem: str) -> Dict[str, Any]:
    dyn_path = SAMPLE_ROOT / "vehicle_dynamics_data" / f"{folder}.csv"
    gaze_path = SAMPLE_ROOT / "vehicle_gaze_data" / f"{folder}.csv"
    label = find_label(folder, file_stem)

    dyn_rows = load_csv_rows(dyn_path)
    timestamps = [float(r["timestamp"]) for r in dyn_rows]
    speed = [float(r["speed"]) for r in dyn_rows]
    acceleration = [float(r["acceleration"]) for r in dyn_rows]
    steering_angle = [float(r["steering_angle"]) for r in dyn_rows]
    braking = [float(r["brake"]) for r in dyn_rows]

    duration = max(timestamps) - min(timestamps) if timestamps else 0.0
    interval = (
        (sum(b - a for a, b in zip(timestamps, timestamps[1:])) / (len(timestamps) - 1))
        if len(timestamps) > 1
        else 0.0
    )

    gaze_rows = load_csv_rows(gaze_path)
    gaze_series = [{"t": float(r["timestamp"]), "object": r.get("class", "unknown")} for r in gaze_rows]
    gaze_sampled = gaze_series[:: max(1, len(gaze_series) // 30)] if gaze_series else []

    action_set = list(dict.fromkeys(label.get("recommended_actions", []) + DEFAULT_ACTION_POOL))

    payload: Dict[str, Any] = {
        "duration": round(duration, 2),
        "interval": round(interval, 3),
        "speed": sample_series(speed),
        "acceleration": sample_series(acceleration),
        "steering_angle": sample_series(steering_angle),
        "braking": sample_series(braking),
        "object_fixations": gaze_sampled,
        "autonomous_mode": label.get("autonomous_mode", []),
        "action": action_set,
    }
    return payload


def build_prompt(payload: Dict[str, Any]) -> str:
    # Modify prompt here if needed.
    instruction = INSTRUCTION.format(**payload)
    return f"{one_shot}\n\n{instruction}"


def download_video(url: str, tmp_dir: Path) -> Path:
    resp = requests.get(url, stream=True, timeout=30)
    resp.raise_for_status()
    tmp_path = tmp_dir / f"video_{uuid.uuid4().hex}.mp4"
    with tmp_path.open("wb") as f:
        for chunk in resp.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
    return tmp_path


def _save_frame(frame, tmp_dir: Path, idx: int) -> Path:
    from PIL import Image
    import io

    img = Image.fromarray(frame)
    buf = io.BytesIO()
    img.save(buf, format="JPEG")
    path = tmp_dir / f"frame_{idx}.jpg"
    path.write_bytes(buf.getvalue())
    return path


def sample_frames(video_path: Path, tmp_dir: Path, num_frames: int) -> List[Path]:
    frames: List[Path] = []
    try:
        meta = iio.immeta(video_path, plugin="pyav")
        total = int(meta.get("n_frames") or 0)
    except Exception:
        total = 0
    if total <= 0:
        for idx, frame in enumerate(iio.imiter(video_path, plugin="pyav")):
            if idx >= num_frames:
                break
            frames.append(_save_frame(frame, tmp_dir, idx))
        return frames

    if num_frames <= 1:
        targets = {0}
    else:
        step = (total - 1) / (num_frames - 1)
        targets = {int(round(i * step)) for i in range(num_frames)}

    for idx, frame in enumerate(iio.imiter(video_path, plugin="pyav")):
        if idx in targets:
            frames.append(_save_frame(frame, tmp_dir, idx))
        if len(frames) >= num_frames:
            break
    return frames


def qwen_chat(
    client: OpenAI,
    model: str,
    prompt: str,
    images: List[Path],
    max_tokens: int = 1200,
    temperature: float = 0.3,
) -> str:
    resp = client.chat.completions.create(
        model=model,
        messages=[
            {
                "role": "user",
                "content": prompt,
                "images": [str(p) for p in images],
            }
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return resp.choices[0].message.content.strip()


def parse_reward(eval_report: str) -> float:
    # Try strict JSON first
    try:
        data = json.loads(eval_report)
        val = data.get("scalar_reward")
        if val is None:
            val = data.get("total_score")
        return float(val)
    except Exception:
        pass
    # Fallback: regex extract scalar_reward / total_score anywhere in text
    for key in ("scalar_reward", "total_score"):
        m = re.search(rf'"{key}"\s*:\s*([0-9]+(?:\.[0-9]+)?)', eval_report)
        if m:
            try:
                return float(m.group(1))
            except Exception:
                continue
    return 0.0


def run_iter(
    video_path: Path,
    payload: Dict[str, Any],
    reward_threshold: float,
    max_iter: int,
    model: str,
    base_url: str,
    num_frames: int,
) -> str:
    video_uri = str(video_path.resolve())
    question = build_prompt(payload)

    history: List[Dict[str, Any]] = []

    client = OpenAI(api_key="ollama", base_url=base_url)

    with tempfile.TemporaryDirectory() as td:
        tmp_dir = Path(td)
        frames = sample_frames(video_path, tmp_dir, num_frames)
        if not frames:
            raise RuntimeError("No frames sampled; check video or imageio[pyav].")

        draft = qwen_chat(client, model, question, images=frames, max_tokens=900)
        eval_prompt = (
            eval_reward_prompt_template.format(
                question=question,
                context=json.dumps(payload, ensure_ascii=False, indent=2),
                initial_answer=draft,
            )
            + "\nReturn ONLY the JSON object, no extra text."
        )
        eval_report = qwen_chat(client, model, eval_prompt, images=frames, max_tokens=700)
        reward = parse_reward(eval_report)
        history.append({"answer": draft, "eval_report": eval_report, "reward": reward})

        current_answer = draft
        current_eval = eval_report
        current_reward = reward

        for _ in range(max_iter):
            if current_reward >= reward_threshold:
                break
            refine_prompt = (
                "You are improving the previous driving evaluation report.\n"
                f"Instruction + examples:\n{question}\n\n"
                f"Previous report:\n{current_answer}\n\n"
                f"Critic feedback:\n{current_eval}\n\n"
                "Rewrite to fix the issues noted by the critic. Keep exactly five headings "
                "(Scene Description, Driver's Attention, Human-Machine Interaction, Evaluation & Suggestions, Recommended Actions). "
                "Recommended Actions must list the top 3 items chosen from this set only: "
                f"{payload.get('action', [])}. "
                "Be concise and evidence-grounded."
            )
            improved = qwen_chat(client, model, refine_prompt, images=frames, max_tokens=800, temperature=0.2)

            eval_prompt = (
                eval_reward_prompt_template.format(
                    question=question,
                    context=json.dumps(payload, ensure_ascii=False, indent=2),
                    initial_answer=improved,
                )
                + "\nReturn ONLY the JSON object, no extra text."
            )
            eval_report = qwen_chat(client, model, eval_prompt, images=frames, max_tokens=700)
            reward = parse_reward(eval_report)
            history.append({"answer": improved, "eval_report": eval_report, "reward": reward})

            current_answer, current_eval, current_reward = improved, eval_report, reward

        format_prompt = (
            "Rewrite the final driver evaluation so it strictly follows the template with five headings "
            "(Scene Description, Driver's Attention, Human-Machine Interaction, Evaluation & Suggestions, Recommended Actions). "
            "Use only the provided action set and pick the top three actions, ranked. "
            "Keep concise, avoid extra lists outside the actions section.\n"
            f"Action set: {payload.get('action', [])}\n"
            f"Draft to fix:\n{current_answer}\n"
            "If any section is missing, add it based on available information."
        )
        formatted_answer = qwen_chat(client, model, format_prompt, images=frames, max_tokens=600, temperature=0.2)

        return formatted_answer


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Iterative driving report with local Ollama (VideoAgent-style).")
    src = parser.add_mutually_exclusive_group(required=False)
    src.add_argument("--video-path", help="Local video path (mp4).")
    src.add_argument("--video-url", help="Remote video URL (mp4).")
    parser.add_argument("--folder", default="01-1", help="Sample folder name.")
    parser.add_argument("--file-stem", default="start_at_min02sec03", help="File stem without extension.")
    parser.add_argument("--num-frames", type=int, default=DEFAULT_NUM_FRAMES, help="Number of sampled frames.")
    parser.add_argument("--model", default=DEFAULT_MODEL, help="Ollama model name.")
    parser.add_argument("--base-url", default=DEFAULT_BASE_URL, help="Ollama OpenAI-compatible endpoint.")
    parser.add_argument("--output", default="", help="If set, save result to this file (UTF-8).")
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    default_video = SAMPLE_ROOT / args.folder / f"{args.file_stem}.mp4"

    with tempfile.TemporaryDirectory() as td:
        tmp_dir = Path(td)
        if args.video_path:
            video_path = Path(args.video_path)
        elif args.video_url:
            video_path = download_video(args.video_url, tmp_dir)
        else:
            video_path = default_video

        if not video_path.exists():
            raise FileNotFoundError(f"Video not found: {video_path}")

        payload = build_payload(args.folder, args.file_stem)
        result = run_iter(
            video_path,
            payload,
            reward_threshold=DEFAULT_REWARD_THRESHOLD,
            max_iter=DEFAULT_MAX_ITER,
            model=args.model,
            base_url=args.base_url,
            num_frames=args.num_frames,
        )
    if args.output:
        out_path = Path(args.output)
        out_path.parent.mkdir(parents=True, exist_ok=True)
        out_path.write_text(result, encoding="utf-8")
    else:
        print(result)
